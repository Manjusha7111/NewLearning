import google.cloud.storage
from google.cloud import bigquery
import re
import csv
from  collections import defaultdict
 
 
# Use the configuration in the script
project_id = 'tnt01-odycda-bld-01-1b81'
 
# Initialize GCS and BigQuery clients
client = google.cloud.storage.Client(project=project_id)
 
 
# Step 1: List files in the specified "folder" in the GCS bucket
bucket_name = 'tnt01-odycda-bld-01-stb-eu-rawzone-52fd7181'
 
blob_name ='INTERNAL/MFVS/MIP/CDM/ref.txt'
 
bucket = client.get_bucket(bucket_name)
blob = bucket.blob(blob_name)
 
 
#read content from gcs bucket
 
content = blob.download_as_text()
lines = content.splitlines()
 
reader = csv.reader(lines)
 
next(reader)
 
#process data
 
data = defaultdict(list)
column_counter = defaultdict(int)
headers = defaultdict(list)
 
for row in reader:
    if not row:
        continue

    table_name = row[0].replace(' ','_').lower()
    column_name = row[1].replace(' ','_').lower()
    column_counter[column_name] +=1
    code = f"{row[2]}{column_counter[column_name]:03d}"
 
 
    name = row[3]
    description = row[3]
    effective_from_date ='01-Jan-2001'
    effective_to_date='31-Dec-9999'
    data[table_name].append([code,name,description,effective_from_date,effective_to_date])
 
    if table_name not in headers:
       headers[table_name] =[column_name,"name","description","effective_from_date","effective_to_date"]
#write output to GCS bucket
 
for table_name,rows in data.items():
    table_name_raw = f"{table_name}_raw"
    gcs_file_path = f'egress/{table_name_raw}.csv'

    output = []
    output.append(",".join(headers[table_name]))
    for row in rows:
        output.append(",".join(row))
    csv_data = "\n".join(output)

    blob = bucket.blob(gcs_file_path)
    blob.upload_from_string(csv_data,content_type='text/csv')