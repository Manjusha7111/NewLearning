from google.cloud import bigquery
client=bigquery.Client(project="bfsi-ai-data-usecase-98uh7")
q="SELECT * FROM `bfsi-ai-data-usecase-98uh7.BITest.dealer` LIMIT 10 "
query_job=client.query(q)
res=query_job.result()
for i in res:
    print(i)
    print(i.Brand)

____________________________________________

SELECT Brand FROM `bfsi-ai-data-usecase-98uh7.BITest.dealer` where colour='Red'  LIMIT 10
___________________________________________
dataset creation:


from google.cloud import bigquery
# Construct a BigQuery client object.
client = bigquery.Client()
# Set dataset_id to the name of the dataset
dataset_id = "bfsi-ai-data-usecase-98uh7.BITest1"
dataset = bigquery.Dataset(dataset_id)
# Location must match that of the base dataset.
dataset.location = "US"
# Send the dataset to the API for creation.

# Raises google.api_core.exceptions.Conflict if the Dataset already
# exists within the project.
dataset = client.create_dataset(dataset)
print("Created dataset {}.{}".format(client.project, dataset.dataset_id))
____________________________________________________________
table:



from google.cloud import bigquery


# Construct a BigQuery client object.

client = bigquery.Client()


# Set table_id to the ID of the table to create.

table_id = "bfsi-ai-data-usecase-98uh7.BITest1.demo1"



schema = [

    bigquery.SchemaField("name", "STRING"),

    bigquery.SchemaField("age", "INTEGER"),

]


table = bigquery.Table(table_id, schema=schema)


table = client.create_table(table)


print("Created table {}.{}".format(
    table.project, table.dataset_id, table.table_id))
_____________________________________________________

insert data:


******************************************


from google.cloud import bigquery

table_id = "bfsi-ai-data-usecase-98uh7.BITest1.demo1"
client = bigquery.Client()
rows_to_insert = [

    {'name': 'John', 'age': 30},

    {'name': 'Jane', 'age': 25},

]
errors = client.insert_rows_json(table_id, rows_to_insert)
if errors == []:

    print("Loaded rows successfully")

else:

    print("Errors: ", errors)
_____________________________________________________________


________________________________________________________
from google.cloud import bigquery
client = bigquery.Client()
table_id = "bfsi-ai-data-usecase-98uh7.BITest1.demo1"
rows_to_insert = [

    {'name': 'John', 'age': 30},

    {'name': 'Jane', 'age': 25},

]
errors = client.insert_rows(table_id, rows_to_insert)
if errors == []:

    print("Loaded rows successfully")

else:

    print("Errors: ", errors)
________________________________________________________

The "u" prefix before a string denotes that it is a Unicode string literal.Unicode is a standard for encoding characters in a way that can be understood by computers. 
_____________________________________________________________________________________________
working code:


from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT
  date,
  GENERATE_UUID() AS Unique_Key,
  ROW_NUMBER() OVER() AS Date_Id,
  FORMAT_DATE("%Y/%B/%d",date) AS Full_date,
  EXTRACT(DAY FROM date) AS Year_day,
  EXTRACT(WEEK FROM date) AS Year_Week,
  EXTRACT(MONTH FROM date) AS Year_month,
  EXTRACT(YEAR FROM date) AS Year,
  FORMAT_DATE("%A", date) as Week_Full_Week,
  FORMAT_DATE("%B", date) as Month_Full_Week,
  EXTRACT(QUARTER FROM date) AS year_Qtr,
  EXTRACT(month from date_add(date, interval (12-4)+1 month)) as Fiscal_Month,
  case when EXTRACT(MONTH FROM  date)>=4 then EXTRACT(YEAR FROM  date)
  ELSE EXTRACT(YEAR FROM  date)-1
  END As FISCAL_YEAR,
  DATE_ADD(date, INTERVAL 100 YEAR) AS After_hun_yrs,
  DATE_SUB(date, INTERVAL 100 YEAR) AS Before_hun_yrs,
  FORMAT_DATE("%Y/%B",date) AS Yearmonth,
  FORMAT_DATE("%Y%m", date) as Y_m,
  FORMAT_DATE("%y%b%d",date) AS Full_date1,
FROM UNNEST(GENERATE_DATE_ARRAY('1900-01-01', '2023-01-31')) AS date 
ORDER BY Date"""

job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest1.rough", write_disposition='WRITE_TRUNCATE')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')

___________________________________________

SELECT
 *
FROM
  `bfsi-ai-data-usecase-98uh7.BITest.customer` as c
INNER JOIN
  `bfsi-ai-data-usecase-98uh7.BITest.sales` as s
ON
  c.Cust_id=s.Cust_id;
________________________________
from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT
 *
FROM
  `qwiklabs-gcp-04-343c501b68a0.n.customer` as c
INNER JOIN
  `qwiklabs-gcp-04-343c501b68a0.n.sales` as s
ON
  c.Cust_id=s.Cust_id;"""

job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest1.rough", write_disposition='WRITE_TRUNCATE')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')


table = client.create_table(table)


print("Created table {}.{}".format(
    table.project, table.dataset_id, table.table_id))
____________________________________________

adding table_id:

from google.cloud import bigquery
 
# Initialize a client
client = bigquery.Client()
 #project_id.dataset_id.table_id
# Define your project, dataset, and table names
project_id = 'bfsi-ai-data-usecase-98uh7'
dataset_id = 'bfsi-ai-data-usecase-98uh7.BITest1'
table_id = 'bfsi-ai-data-usecase-98uh7.BITest1.rough6011'
 
query_file_path = '/home/balathoti_manjusha/python/my_query.sql'
 
# Read the SQL query from the file
with open(query_file_path, 'r') as query_file:
    query = query_file.read()
 
# Construct the SQL query
query = query.format(project_id=project_id, dataset_id=dataset_id, table_id=table_id)
job_config = bigquery.QueryJobConfig(
    destination=f"{table_id}", write_disposition='WRITE_APPEND')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')

_________________________________________________

added json:

import json
from google.cloud import bigquery
 
# Initialize a client

 
query_file_path = '/home/balathoti_manjusha/python/my_query.sql'

# Read the JSON config file
with open('a.json', 'r') as json_file:
    config = json.load(json_file)

client = bigquery.Client()
 #project_id.dataset_id.table_id
# Define your project, dataset, and table names
project_id = config['project_id']
dataset_id = config['dataset_id']
table_id = config['table_id']
 
# Read the SQL query from the file
with open(query_file_path, 'r') as query_file:
    query = query_file.read()
 
# Construct the SQL query
query = query.format(project_id=project_id, dataset_id=dataset_id, table_id=table_id)
job_config = bigquery.QueryJobConfig(
    destination=f"{table_id}", write_disposition='WRITE_APPEND')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')

_____________________________________

parametrized p,d,t,sql::

import json
from google.cloud import bigquery
 
# Read the JSON config file
with open('a.json', 'r') as json_file:
    config = json.load(json_file)

# Initialize a client
client = bigquery.Client()
 #project_id.dataset_id.table_id
# Define your project, dataset, and table names
project_id = config['project_id']
dataset_id = config['dataset_id']
table_id = config['table_id']
sql_query = config['sql_query']
query_file_path =f"{sql_query}"
# Read the SQL query from the file
with open(query_file_path, 'r') as query_file:
    query = query_file.read()

 
# Construct the SQL query
query = query.format(project_id=project_id, dataset_id=dataset_id, table_id=table_id)
job_config = bigquery.QueryJobConfig(
    destination=f"{table_id}", write_disposition='WRITE_APPEND')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')

________________________________________________


final:b.python

import json
from google.cloud import bigquery
 
# Read the JSON config file
with open('a.json', 'r') as json_file:
    config = json.load(json_file)

# Initialize a client
client = bigquery.Client()
#project_id.dataset_id.table_id
# Define your project, dataset, and table names
project_id = config['project_id']
dataset_id = config['dataset_id']
table_id = config['table_id']
sql_query = config['sql_query']
query_file_path =sql_query

# Read the SQL query from the file
with open(query_file_path, 'r') as query_file:
    query = query_file.read()

 
# Construct the SQL query
query = query.format(project_id=project_id, dataset_id=dataset_id, table_id=table_id)
job_config = bigquery.QueryJobConfig(destination=table_id,write_disposition="WRITE_TRUNCATE")


query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')



a.json

{

    "project_id" : "bfsi-ai-data-usecase-98uh7",
    "dataset_id" : "bfsi-ai-data-usecase-98uh7.BITest1",
    "table_id"   :"bfsi-ai-data-usecase-98uh7.BITest1.super1000",
    "sql_query"  : "/home/balathoti_manjusha/python/my_query.sql"
    
}



query :

my_query.sql


___________________________________________________________________________________

#Deleting previously existing data and overwriting with the new data

from google.cloud import bigquery

client = bigquery.Client()

table_name = "qwiklabs-gcp-01-3f619b5b15d2.BITest1.demo1"

job_config = bigquery.LoadJobConfig(
    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
    schema=[
        bigquery.SchemaField('name', 'STRING', mode='REQUIRED'),
        bigquery.SchemaField('age', 'INTEGER', mode='REQUIRED')
    ]
)

data = [
    {
        "name": "manju",
        "age": 10
    },
    {
        "name": "Jane",
        "age": 5
    }
]


job = client.load_table_from_json(
    data,
    table_name,
    job_config=job_config,

)

job.result()

print("Data overwritten")

____________________________________________________________________________

#completly deleting a table from bq:

from google.cloud import bigquery
 
client = bigquery.Client()
 
# Set table name
table_id = "bfsi-ai-data-usecase-98uh7.BITest1.NEW1"  
 
# Confirm table exists
table = client.get_table(table_id)  
print(f"Table {table_id} exists, deleting table...")  
 
# Delete table
client.delete_table(table_id)  
 
# Confirm deletion
try:
    client.get_table(table_id)
    print(f"Table {table_id} still exists!")
except:
    print(f"Successfully deleted table {table_id}")
_______________________________________________________________________

#updating schema:

from google.cloud import bigquery
 
client = bigquery.Client()
 
# Table ID
table_id = "bfsi-ai-data-usecase-98uh7.BITest1.demo1"
 
# Get table
table = client.get_table(table_id)  
 
# Print schema before update
print("Original Schema:")
for field in table.schema:
    print("{} : {}".format(field.name, field.field_type))
 
# Add new column    
new_schema = table.schema[:]   
new_schema.append(bigquery.SchemaField("height", "STRING"))  
 
# Update schema
table.schema = new_schema       
 
# Submit schema update
table = client.update_table(table, ['schema'])
 
# Print new schema  
print("Updated Schema:")
for field in table.schema:
    print("{} : {}".format(field.name, field.field_type))

o/p:
Original Schema:
name : STRING
age : INTEGER

Updated Schema:
name : STRING
age : INTEGER
height : STRING

___________________________________________________________

creating view

from google.cloud import bigquery
client = bigquery.Client()
view_query = """
    CREATE VIEW `bfsi-ai-data-usecase-98uh7.BITest2.v101` AS
    SELECT * or order_id,order_date
    FROM `bfsi-ai-data-usecase-98uh7.BITest1.test`  
"""
# Create the view
query_job = client.query(view_query)
query_job.result()
print('created')
__________________________________________________________

new calender semantics data: except last one all good

from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT
  ROW_NUMBER() OVER() AS DATE_ID,
  FORMAT_DATE("%d/%m/%Y",date) AS FULL_DATE,
  EXTRACT(YEAR FROM date) AS YEAR,
  FORMAT_DATE("%Y%m",date)AS YEAR_MONTH,
  EXTRACT(WEEK FROM date) AS YEAR_WEEK,
  EXTRACT(DAY FROM date) AS YEAR_DAY,
  case when EXTRACT(MONTH FROM  date)>=4 then EXTRACT(YEAR FROM  date)
  ELSE EXTRACT(YEAR FROM  date)-1
  END As FISCAL_YEAR,
  EXTRACT(QUARTER FROM date) AS FISCAL_QTR,
  EXTRACT(MONTH FROM date) AS MONTH,
  FORMAT_DATE("%b", date) as Month_NAME,
  FORMAT_DATE("%A", date) as WEEK_NAME,

FROM UNNEST(GENERATE_DATE_ARRAY('2023-01-01', '2023-03-31')) AS date 
ORDER BY Date"""

job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest2.WEEKROUGH", write_disposition='WRITE_TRUNCATE')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')
____________________________________________________________
week day or not:::

from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT *,
  case
  WHEN  WEEK_NAME='Sunday' then 'N'
  WHEN  WEEK_NAME='Monday' then 'Y'
  WHEN WEEK_NAME= 'Tuesday' then 'Y'
  WHEN  WEEK_NAME='Wednesday' then 'Y'
  WHEN  WEEK_NAME='Thursday' then 'Y'
  WHEN  WEEK_NAME='Friday' then 'Y'
  WHEN  WEEK_NAME='Saturday' then 'Y'
  end as DAY_IS,    
FROM `bfsi-ai-data-usecase-98uh7.BITest2.WEEKROUGH` """
job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest2.WEEKROUGH", write_disposition='WRITE_TRUNCATE')
________________________________________________________________________________



from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT *,
  case
  WHEN  WEEK_NAME='Sunday' then 'N'
  WHEN  WEEK_NAME='Monday' then 'Y'
  WHEN WEEK_NAME= 'Tuesday' then 'Y'
  WHEN  WEEK_NAME='Wednesday' then 'Y'
  WHEN  WEEK_NAME='Thursday' then 'Y'
  WHEN  WEEK_NAME='Friday' then 'Y'
  WHEN  WEEK_NAME='Saturday' then 'Y'
  end as WEEKS,    
FROM `bfsi-ai-data-usecase-98uh7.BITest2.WEEKS` """
job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest2.WEEKS", write_disposition='WRITE_TRUNCATE')

query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')

_______________________________________________________________

new calender code:

from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT
  ROW_NUMBER() OVER() AS DATE_ID,
  FORMAT_DATE("%d/%m/%Y",date) AS FULL_DATE,
  EXTRACT(YEAR FROM date) AS YEAR,
  FORMAT_DATE("%Y%m",date)AS YEAR_MONTH,
  EXTRACT(WEEK FROM date) AS YEAR_WEEK,
  EXTRACT(DAY FROM date) AS YEAR_DAY,
  case when EXTRACT(MONTH FROM  date)>=4 then EXTRACT(YEAR FROM  date)
  ELSE EXTRACT(YEAR FROM  date)-1
  END As FISCAL_YEAR,
  EXTRACT(QUARTER FROM date) AS FISCAL_QTR,
  EXTRACT(MONTH FROM date) AS MONTH,
  FORMAT_DATE("%b", date) as Month_NAME,
  FORMAT_DATE("%A", date) as WEEK_NAME,
  case
  WHEN  FORMAT_DATE("%A", date)='Sunday' then 'N'
  WHEN  FORMAT_DATE("%A", date)='Monday' then 'Y'
  WHEN FORMAT_DATE("%A", date) = 'Tuesday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Wednesday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Thursday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Friday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Saturday' then 'Y'
  end as IS_DAY_WEEKDAY
FROM UNNEST(GENERATE_DATE_ARRAY('1893-01-01', '2023-12-31')) AS date
ORDER BY Date
 
"""
 
job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BITest2.DIM_CAL", write_disposition='WRITE_APPEND')
 
query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')


____________________________________________________________

few changes to cal-datatypes,year-mon,week, day


from google.cloud import bigquery
client = bigquery.Client()
query = """SELECT
  ROW_NUMBER() OVER() AS DATE_ID,
  PARSE_DATE("%D", FORMAT_DATE("%D", date)) AS FULL_DATE,
  EXTRACT(YEAR FROM date) AS YEAR,
  EXTRACT(YEAR FROM date) * 100 + EXTRACT(MONTH FROM date) AS YEAR_MONTH,
  EXTRACT(YEAR FROM date) * 100 + EXTRACT(WEEK FROM date) AS YEAR_WEEK,
  EXTRACT(YEAR FROM date) * 1000 + EXTRACT(DAYOFYEAR FROM date) AS YEAR_DAY,
  case when EXTRACT(MONTH FROM  date)>=4 then EXTRACT(YEAR FROM  date)
  ELSE EXTRACT(YEAR FROM  date)-1
  END As FISCAL_YEAR,
  EXTRACT(QUARTER FROM date) AS FISCAL_QTR,
  EXTRACT(MONTH FROM date) AS MONTH,
  FORMAT_DATE("%b", date) as Month_NAME,
  FORMAT_DATE("%A", date) as DAY_NAME,
  case
  WHEN  FORMAT_DATE("%A", date)='Sunday' then 'N'
  WHEN  FORMAT_DATE("%A", date)='Monday' then 'Y'
  WHEN  FORMAT_DATE("%A", date) = 'Tuesday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Wednesday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Thursday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Friday' then 'Y'
  WHEN  FORMAT_DATE("%A", date)='Saturday' then 'Y'
  end as IS_DAY_WEEKDAY
FROM UNNEST(GENERATE_DATE_ARRAY('1893-01-01', '2023-12-31')) AS date
ORDER BY Date
 
"""
 
job_config = bigquery.QueryJobConfig(
    destination="bfsi-ai-data-usecase-98uh7.BiDimension.rough", write_disposition='WRITE_TRUNCATE')
 
query_job = client.query(query, job_config=job_config)
query_job.result()  # Waits for job to complete
print('Query results loaded back into mydataset.mytable')   




PARSE_DATE("%D", FORMAT_DATE("%D", date)) AS FULL_DATE,
  CAST(PARSE_DATE("%D", FORMAT_DATE("%D", date))as DATE) as changing,    


#PARSE_DATE("%d/%m/%Y", FORMAT_DATE("%D", date)) AS fulldateEE,   


FORMAT_DATE("%d/%m/%y",DATE) AS Fd1,
  PARSE_DATE("%d/%m/%y", FORMAT_DATE("%d/%m/%y", date)) AS fd2,
  FORMAT_DATE("%d/%m/%y", date) AS fd3,
  CAST(FORMAT_DATE("%d/%m/%Y",date)as integer )AS fd5,
  CAST(FORMAT_DATE("%Y%m", date)as date) as checking,
  CAST(FORMAT_DATE("%Y%m", date)as INTEGER) as changingdatatype,     