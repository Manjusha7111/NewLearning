
INPUT:
Name,Age,Score
John,25,85
Alice,30,92
Bob,28,75
Alice,32,94

OUTPUT:
Name,Age,Score,Valid
John,25,85,Valid
Alice,30,92,Valid
Bob,28,75,Valid
Alice,32,94,Invalid: Volume exceeds the limit
__________________________________________________
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
 
class ValidateData(beam.DoFn):
    def __init__(self, unique_column):
        self.unique_column = unique_column
        self.unique_values = set()
 
    def process(self, element):
        # Split the CSV line into individual columns
        name, age, score = element.split(',')
 
        # Schema Validation
        if len(element.split(',')) != 3:
            yield f'{element},Invalid: Incorrect number of columns'
            return
 
        # Volume Checking
        if len(self.unique_values) >= 4:
            yield f'{element},Invalid: Volume exceeds the limit'
            return
 
        # Unique Column Check
        if self.unique_column == 'Name':
            if name in self.unique_values:
                yield f'{element},Duplicate: Duplicate entry for column {self.unique_column}'
            else:
                self.unique_values.add(name)
                yield f'{element},Valid'
        else:
            yield f'{element},Invalid: Unsupported unique column'
 
def run_data_validation_pipeline(input_csv, output_csv, unique_column):
    options = PipelineOptions()
    with beam.Pipeline(options=options) as p:
        validated_data = (
            # Read input CSV file
p | 'ReadInputCSV' >> beam.io.ReadFromText(input_csv)
            # Apply the ValidateData transform
            | 'ValidateData' >> beam.ParDo(ValidateData(unique_column))
            # Write the validated data to an output CSV file
| 'WriteValidatedData' >> beam.io.WriteToText(output_csv)
        )
 
if __name__ == '__main__':
    # Specify input and output file paths
    input_csv = '/home/balathoti_manjusha/python/apache/eg4qualitychecks/data.csv'
    output_csv = '/home/balathoti_manjusha/python/apache/eg4qualitychecks/validated_data.csv'
    
    # Specify the column for unique checks
    unique_column = 'Name'
 
    # Run the data validation pipeline
    run_data_validation_pipeline(input_csv, output_csv, unique_column)
_____________________________________________________________
EXPLAINATION:
steps:
We will perform three types of validations:
1.Schema Validation: Check if the columns in the CSV file match the expected schema.
2.Volume Checking: Ensure that the number of rows does not exceed a certain limit.
3.Unique Column Check: Check for duplicate entries in a specified column.


#Define a custom DoFn class for data validation
class ValidateData(beam.DoFn):



#Initializing Unique Values Set:
#Initialize the unique values set in the constructor
def __init__(self, unique_column):
        self.unique_column = unique_column
        self.unique_values = set()



#Processing Each CSV Line:
#Split each CSV line into individual columns
#The process method checks for schema validation, volume checking, and unique column checks based on the specified unique_column
 def process(self, element):
        # Split the CSV line into individual columns
        name, age, score = element.split(',')




#Schema Validation:
#Check if the number of columns is as expected
if len(element.split(',')) != 3:
            yield f'{element},Invalid: Incorrect number of columns'
            return


#volume checking:
check if num of rows exceeds a certain limiy.
if len(self.unique_values) >= 4:
            yield f'{element},Invalid: Volume exceeds the limit'
            return


#unique col check:
# check duplicates in specified unique col
if self.unique_column == 'Name':
            if name in self.unique_values:
                yield f'{element},Duplicate: Duplicate entry for column {self.unique_column}'
            else:
                self.unique_values.add(name)
                yield f'{element},Valid'
        else:
            yield f'{element},Invalid: Unsupported unique column'

#run pipeline:
def run_data_validation_pipeline(input_csv, output_csv, unique_column):
    options = PipelineOptions()
    with beam.Pipeline(options=options) as p:
        validated_data = (
            # Read input CSV file
p | 'ReadInputCSV' >> beam.io.ReadFromText(input_csv)
            # Apply the ValidateData transform
            | 'ValidateData' >> beam.ParDo(ValidateData(unique_column))
            # Write the validated data to an output CSV file
| 'WriteValidatedData' >> beam.io.WriteToText(output_csv)
        )



run script:
if __name__ == '__main__':
    # Specify input and output file paths
    input_csv = '/home/balathoti_manjusha/python/apache/eg4qualitychecks/data.csv'
    output_csv = '/home/balathoti_manjusha/python/apache/eg4qualitychecks/validated_data.csv'
    
    # Specify the column for unique checks
    unique_column = 'Name'
 
    # Run the data validation pipeline
    run_data_validation_pipeline(input_csv, output_csv, unique_column)





