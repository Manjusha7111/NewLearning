BIG QUERY: 
Purpose:Large scale data warehouse that helps you manage and analyze your data with built-in features like 
machine learning, geospatial analysis, and business intelligence. 
-It is fully managed serverless data warehouse so it helps us to focus on scalable analysis over petabytes of data.
-Supports SQLworkbench
-It provides both  batch and streaming modes to load data.
-speed (can fetch billions in secs)
-BQ comes under paas that supports quering using sql.

3 parts -storing data ,ingestion,quering.



-SELECT * FROM `bfsi-ai-data-usecase-98uh7.GCP_Test.demo-car` LIMIT 1000

-SELECT Manufacturer,Model
FROM `bfsi-ai-data-usecase-98uh7.GCP_Test.File1` LIMIT 1000

-SELECT Model,avg(Yourvalue)
FROM `bfsi-ai-data-usecase-98uh7.GCP_Test.File1` group by Model

-SELECT Manufacturer,Avg(Average_peer), 
FROM `bfsi-ai-data-usecase-98uh7.GCP_Test.File1` 
group by Manufacturer 
having Manufacturer='Jeep'
 LIMIT 1000
---------------



3)bq --location=us-central1 mk\
--dataset \
--default_table_expiration=3600 \
--description="My dataset" \
bfsi-ai-data-usecase-98uh7:BITest123 


bq --location=us-east1 mk\
--table \
--default_table_expiration=36000 \
--description="My table" \
qwiklabs-gcp-00-2dd738b92b3d:BITest1:bi_testing1 



4 ways of creation:
------------------
*Big Query console
*Query
*command line
*SDK-p,j,g

1)




2)
data set creation:
-----------------
create schema biii_testing
options(location='asia-south2')

table:
------
create table biii_testing.demo1
(
  gender string,
  name string,
  id integer,

)
OPTIONS(
  expiration_timestamp=TIMESTAMP "2023-12-12 00:00:00 UTC",
  description="MY TABLE"
);


insert into `biii_testing.demo1` values('Female','A',5232),('Male','B',2232),('Female','C',8732)


schedule:
SELECT Model FROM `bfsi-ai-data-usecase-98uh7.GCP_Test.emp` LIMIT 1000
UTC-HH:MM-20:10
-----------------------------------------------------------





Terraform:can be accessed in any of the cloud platform that are present in market.
1.terraform 2.google cloud acc 3.editor
-service acc needed-editor-key-(p12,json) format
execution:
1.terraform init-initialize
2.terraform plan-
3.terraform apply-

main.tf-central configuration file for your Terraform project.
define your Google Cloud provider and BigQuery dataset

provider "google" {
credentials = file("path/to/your/credentials.json")
project = "your-project-id"
region = "us-central1"
zone ="us-central1-a"
}

#dataset resource  creation
resource "google_bigquery_dataset" "example_dataset-1" {
dataset_id = "your_dataset_id"
project = "your-project-id"
location = "us-central1"
}

------------------------------------------------------------
#table creation 
provider "google" {
project = "your-project-id"
region = "us-central1"
zone ="us-central1-a"
}

resource "google_bigquery_table" "dataset_table-1" {
dataset_id = "your_dataset_id"
table_id = "your_table_id"
location = "us-central1"
zone ="us-central1-a"

schema {
fields {
name = "name"
type = "STRING"
mode = "NULLABLE"  # null values can be accepted
}

{
name = "id"
type = "INTEGER"
mode = "Requried"
}

{
name = "phno"
type = "INTEGER"
mode = "Requried" #NULL values are not allowed
}
# Add more fields as needed
}
}

------------------------------------------------------
csv:
supports text based format
separated by ,
human readable and easy to generate 
dis:
lacks build in support for large datasets.


Parquet:not human readable
*follows column based format : 
files are organised bu coloumns rather than rows  which saves storage space,speed up performance.
*supports complex datatypes
*good for storing big data of any kind-data processing frameworks like hadoop,spark
*probides compression and encoding techiques.
____________________________________________________

